{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bdefb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\dell\\anaconda3\\lib\\site-packages (7.6.1)\n",
      "Requirement already satisfied: update-checker>=0.18 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from praw) (1.4.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ace7c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display Name: wallstreetbets\n",
      "Title: wallstreetbets\n",
      "Description: Our official Twitter: [@Official_WSB](https://twitter.com/official_wsb)\n",
      "\n",
      "---\n",
      "\n",
      "The rules and submission guidelines are maintained on new Reddit so be sure to check them and make sure you're up to date.\n",
      "\n",
      "* Read the [rules](https://www.reddit.com/r/wallstreetbets/about/rules)\n",
      "\n",
      "* Read the [comment and submission guide](https://www.reddit.com/r/wallstreetbets/wiki/contentguide)\n",
      "\n",
      "* Read the [FAQ](https://www.reddit.com/r/wallstreetbets/wiki/faq) if you're new to both wallstreetbets and trading.\n",
      "\n",
      "---\n",
      "**Join the discord**\n",
      "\n",
      "[WSB Discord](https://discord.gg/rncPUDCJ)\n",
      "\n",
      "**filter by flairs**\n",
      "\n",
      "^Navigate ^WSB |^We ^recommend ^best ^daily ^DD\n",
      ":--|:--     \n",
      "**DD** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3ADD) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3ADD&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3ADD&restrict_sr=on&t=week)\n",
      "**Discussion** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3ADiscussion) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3ADiscussion&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3ADiscussion&restrict_sr=on&t=week)\n",
      "**YOLO** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3AYOLO) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AYOLO&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AYOLO&restrict_sr=on&t=week)\n",
      "**Gain** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3AGain) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AGain&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AGain&restrict_sr=on&t=week)\n",
      "**Loss** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3ALoss) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3ALoss&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3ALoss&restrict_sr=on&t=week)\n",
      "**Shitpost** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3AShitpost) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AShitpost&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AShitpost&restrict_sr=on&t=week)\n",
      "**Meme** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3AMeme) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AMeme&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AMeme&restrict_sr=on&t=week)\n",
      "**Storytime** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3AStorytime) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AStorytime&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AStorytime&restrict_sr=on&t=week)\n",
      "**Satire** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3ASatire) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3ASatire&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3ASatire&restrict_sr=on&t=week)\n",
      "**Options** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3AOptions) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AOptions&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AOptions&restrict_sr=on&t=week)\n",
      "**Futures** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3AFutures) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AFutures&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AFutures&restrict_sr=on&t=week)\n",
      "**Forex** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3AForex) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AForex&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AForex&restrict_sr=on&t=week)\n",
      "**Stocks** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3AStocks) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AStocks&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AStocks&restrict_sr=on&t=week)\n",
      "**Fundamentals** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3AFundamentals) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AFundamentals&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3AFundamentals&restrict_sr=on&t=week)\n",
      "**Technicals** | [All](https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3ATechnicals) / [**Best Daily**](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3ATechnicals&restrict_sr=on&t=day) / [Best Weekly](https://ns.reddit.com/r/wallstreetbets/search?sort=top&q=flair%3ATechnicals&restrict_sr=on&t=week)\n",
      "\n",
      "\n",
      "[Earnings Thread](https://reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3A\"Earnings%20Thread\") - [Daily Thread](https://reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3A\"Daily%20Discussion\")\n",
      "\n",
      "---\n",
      "\n",
      "**Market Trading Hours**\n",
      "\n",
      "|Exchange|Open|Close|\n",
      "|:-------:|--------:|--------:|\n",
      "|[Frankfurt](http://goo.gl/9teCjs) | 9:00 AM | 8:00 PM | \n",
      "|[New York](https://goo.gl/eODOhO) | 9:30 AM  | 4:00 PM|\n",
      "|[CME](http://goo.gl/VZZDPg) | 5:00 PM | 4:15 PM |\n",
      "|[CBOE](http://goo.gl/fMSNBY) | 8:30 AM | 3:15 PM  |\n",
      "|[Tokyo](http://goo.gl/Aiwygk) | 9:00 AM | 3:00 PM  |\n",
      "|[Hong Kong](http://goo.gl/vLR2vh) | 9:30 AM | 4:00 PM  |\n",
      "* Hours respective to their own timezone.\n",
      "\n",
      "[^^source](https://goo.gl/hk9CB4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "# Authorized instance\n",
    "reddit_authorized = praw.Reddit(client_id=\"DlcjXhOBwypg_mZEm6xlwA\",         # your client id\n",
    "                                client_secret=\"kLA5U-4G-eKi47VoCgunW1bnd2lf9Q\",      # your client secret\n",
    "                                user_agent=\"Project\",        # your user agent\n",
    "                                username=\"blackbear199710\",        # your reddit username\n",
    "                                password=\"CWJcwj1010\")        # your reddit password\n",
    "\n",
    "reddit_read_only = praw.Reddit(client_id=\"DlcjXhOBwypg_mZEm6xlwA\",         # your client id\n",
    "                               client_secret=\"kLA5U-4G-eKi47VoCgunW1bnd2lf9Q\",      # your client secret\n",
    "                               user_agent=\"Project\")        # your user agent\n",
    "\n",
    "subreddit = reddit_read_only.subreddit(\"wallstreetbets\")\n",
    " \n",
    "# Display the name of the Subreddit\n",
    "print(\"Display Name:\", subreddit.display_name)\n",
    " \n",
    "# Display the title of the Subreddit\n",
    "print(\"Title:\", subreddit.title)\n",
    " \n",
    "# Display the description of the Subreddit\n",
    "print(\"Description:\", subreddit.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d747a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posts_collection(stock, sort, time):\n",
    "    \n",
    "    posts_dict = {\"Title\": [], \"Post Text\": [],\n",
    "              \"ID\": [], \"Score\": [],\n",
    "              \"Total Comments\": [], \"Post URL\": []\n",
    "              }\n",
    "    for name in stock:\n",
    "        for post in reddit_read_only.subreddit(\"wallstreetbets\").search(name,sort=sort,time_filter=time):\n",
    "    # Title of each post\n",
    "            posts_dict[\"Title\"].append(post.title)\n",
    "     \n",
    "    # Text inside a post\n",
    "            posts_dict[\"Post Text\"].append(post.selftext)\n",
    "     \n",
    "    # Unique ID of each post\n",
    "            posts_dict[\"ID\"].append(post.id)\n",
    "     \n",
    "    # The score of a post\n",
    "            posts_dict[\"Score\"].append(post.score)\n",
    "     \n",
    "    # Total number of comments inside the post\n",
    "            posts_dict[\"Total Comments\"].append(post.num_comments)\n",
    "     \n",
    "    # URL of each post\n",
    "            posts_dict[\"Post URL\"].append(post.url)\n",
    "\n",
    "\n",
    "    # Save posts as dataframe \n",
    "    stock_posts = pd.DataFrame(posts_dict)\n",
    "    return stock_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "278ff83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla = ['tesla','TSLA']\n",
    "apple = ['apple','AAPL']\n",
    "amazon = ['amazon','AMZN']\n",
    "google = ['google','GOOG']\n",
    "meta = ['facebook','META']\n",
    "netfilx = ['netfilx','NFLX']\n",
    "time = 'year'\n",
    "\n",
    "# Collect posts for all six companies\n",
    "\n",
    "Tesla_posts = posts_collection(tesla,\"relevance\", time)\n",
    "Apple_posts = posts_collection(apple,\"relevance\", time)\n",
    "Amazon_posts = posts_collection(amazon,\"relevance\", time)\n",
    "Google_posts = posts_collection(google,\"relevance\", time)\n",
    "Meta_posts = posts_collection(meta,\"relevance\", time)\n",
    "Netfilx_posts = posts_collection(netfilx,\"top\", time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f2b9964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = Apple_posts[\"ID\"]\n",
    "a = Apple_posts[ids.isin(ids[ids.duplicated()])].sort_values(\"ID\")\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5acda6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of data point in the Apple_posts is 200\n",
      "The number of data point after removing duplicates is 194\n",
      "The number of duplicates after cleaning up 0\n"
     ]
    }
   ],
   "source": [
    "print('The number of data point in the Apple_posts is',len(Apple_posts))\n",
    "Apple_posts_clean = Apple_posts.drop_duplicates(subset=['Title'])\n",
    "print('The number of data point after removing duplicates is',len(Apple_posts_clean))\n",
    "ids = Apple_posts_clean[\"ID\"]\n",
    "a = Apple_posts_clean[ids.isin(ids[ids.duplicated()])].sort_values(\"ID\")\n",
    "print('The number of duplicates after cleaning up',len(a))\n",
    "Apple_posts_clean.to_csv(\"Apple_posts_clean.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f9ef72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of data point in the Tesla_posts is 200\n",
      "The number of data point after removing duplicates is 198\n",
      "The number of duplicates after cleaning up 0\n"
     ]
    }
   ],
   "source": [
    "print('The number of data point in the Tesla_posts is',len(Tesla_posts))\n",
    "Tesla_posts_clean = Tesla_posts.drop_duplicates(subset=['Title'])\n",
    "print('The number of data point after removing duplicates is',len(Tesla_posts_clean))\n",
    "ids = Tesla_posts_clean[\"ID\"]\n",
    "a = Tesla_posts_clean[ids.isin(ids[ids.duplicated()])].sort_values(\"ID\")\n",
    "print('The number of duplicates after cleaning up',len(a))\n",
    "Tesla_posts_clean.to_csv(\"Tesla_posts_clean.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3782259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of data point in the Meta_posts is 200\n",
      "The number of data point after removing duplicates is 185\n",
      "The number of duplicates after cleaning up 0\n"
     ]
    }
   ],
   "source": [
    "print('The number of data point in the Meta_posts is',len(Meta_posts))\n",
    "Meta_posts_clean = Meta_posts.drop_duplicates(subset=['Title'])\n",
    "print('The number of data point after removing duplicates is',len(Meta_posts_clean))\n",
    "ids = Meta_posts_clean[\"ID\"]\n",
    "a = Meta_posts_clean[ids.isin(ids[ids.duplicated()])].sort_values(\"ID\")\n",
    "print('The number of duplicates after cleaning up',len(a))\n",
    "Meta_posts_clean.to_csv(\"Meta_posts_clean.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15732355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of data point in the Amazon_posts is 200\n",
      "The number of data point after removing duplicates is 193\n",
      "The number of duplicates after cleaning up 0\n"
     ]
    }
   ],
   "source": [
    "print('The number of data point in the Amazon_posts is',len(Amazon_posts))\n",
    "Amazon_posts_clean = Amazon_posts.drop_duplicates(subset=['Title'])\n",
    "print('The number of data point after removing duplicates is',len(Amazon_posts_clean))\n",
    "ids = Amazon_posts_clean[\"ID\"]\n",
    "a = Amazon_posts_clean[ids.isin(ids[ids.duplicated()])].sort_values(\"ID\")\n",
    "print('The number of duplicates after cleaning up',len(a))\n",
    "Amazon_posts_clean.to_csv(\"Amazon_posts_clean.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc35fbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of data point in the Google_posts is 200\n",
      "The number of data point after removing duplicates is 197\n",
      "The number of duplicates after cleaning up 0\n"
     ]
    }
   ],
   "source": [
    "print('The number of data point in the Google_posts is',len(Google_posts))\n",
    "Google_posts_clean = Google_posts.drop_duplicates(subset=['Title'])\n",
    "print('The number of data point after removing duplicates is',len(Google_posts_clean))\n",
    "ids = Google_posts_clean[\"ID\"]\n",
    "a = Google_posts_clean[ids.isin(ids[ids.duplicated()])].sort_values(\"ID\")\n",
    "print('The number of duplicates after cleaning up',len(a))\n",
    "Google_posts_clean.to_csv(\"Google_posts_clean.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c8276d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of data point in the Google_posts is 100\n",
      "The number of data point after removing duplicates is 100\n",
      "The number of duplicates after cleaning up 0\n"
     ]
    }
   ],
   "source": [
    "print('The number of data point in the Google_posts is',len(Netfilx_posts))\n",
    "Netfilx_posts_clean = Netfilx_posts.drop_duplicates(subset=['Title'])\n",
    "print('The number of data point after removing duplicates is',len(Netfilx_posts_clean))\n",
    "ids = Netfilx_posts_clean[\"ID\"]\n",
    "a = Netfilx_posts_clean[ids.isin(ids[ids.duplicated()])].sort_values(\"ID\")\n",
    "print('The number of duplicates after cleaning up',len(a))\n",
    "Netfilx_posts_clean.to_csv(\"Netfilx_posts_clean.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea4cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
